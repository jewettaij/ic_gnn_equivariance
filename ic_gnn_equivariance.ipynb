{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2610ee-054f-4224-b9e8-1388c7c4662c",
   "metadata": {},
   "source": [
    "# Rotationally equivariant graph neural networks for integrated circuits\n",
    "\n",
    "This file contains some rough notes on how you might apply rotational equivariance to\n",
    "a graph neural network (GNN) model describing an integrated circuit (IC).\n",
    "Any (scalar) predictions you make using a GNN should not change if you rotate the coordinates of the components in the IC in the XY plane.  *(The laws of physics that govern the IC are the same at any orientation.)*  But if you use an ordinary GNN model, this is not guaranteed.\n",
    "\n",
    "This document explains how to exploit this rotational symmetry to improve the accuracy of a GNN model.\n",
    "\n",
    "## Why bother?\n",
    "\n",
    "Many models are rotationally inviariant.  In general, exploiting symmetry can improve model performace and robustness.  Equivariance is commonly used to improve the accuracy of molecular models *(eg. AlphaFold2)* and CNNs.  Here is a visually striking example of the benefits of applying equivariance to CNNs:\n",
    "\n",
    "### Example 1\n",
    "The output of a conventional CNN is not rotationally equivariant.\n",
    "\n",
    "https://github.com/QUVA-Lab/escnn/raw/master/visualizations/conventional_cnn.gif\n",
    "\n",
    "### Example 2\n",
    "Output of an equivariant CNN\n",
    "\n",
    "https://github.com/QUVA-Lab/escnn/raw/master/visualizations/vectorfield.gif\n",
    "\n",
    "*(Credit: Maurice Weiler, Gabriele Cesa)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09055d8e-cbd3-4d14-b6c3-3d591bcf0222",
   "metadata": {},
   "source": [
    "## GNN notation\n",
    "\n",
    "![simple_graph_with_edge_attributes](./images/simple_graph_with_edge_attributes.svg)\n",
    "\n",
    "General form of the update rule that most GNNs use to update their node attributes:\n",
    "\n",
    "$h^\\prime_i\\ =\\ \\mathcal{U}\\left(h_i \\ , \\ \\bigoplus\\limits_{j\\in \\mathcal{N}(i)}\\ \\mathcal{m}(h_i,\\ h_j,\\ e_{ij}) \\right)$\n",
    "\n",
    "Where:\n",
    " - $h_i$  are node attributes (for circuit components, their terminals, and points of interest along the wires in the circuit).\n",
    "\n",
    " - $e_{ij}$  are (optional) edge attributes between nodes $i$ and $j$.  ($e_{ij}$ might encode whether nodes $i$ and $j$ are nearby and/or directly connected by a wire.)\n",
    "\n",
    " - $\\mathcal{N}(i)$ are the neighbors of node $i$.\n",
    "\n",
    " - $\\bigoplus$  is a message aggregator *(typically a sum, $\\sum_{j\\in \\mathcal{N}(i)}$)*\n",
    "\n",
    " - $\\mathcal{m}()$  calculates the message from node $j$ to node $i$.  *(Typically an MLP.)*\n",
    "\n",
    " - $\\mathcal{U}()$  is the node update function, combining the aggregated messages with the attributes for node $i$.  *(Typically an MLP.)*\n",
    "\n",
    "## Introducing geometry\n",
    "In general, the 3D coordinates of each node ($\\vec{x}_i, \\vec{x}_j$) could affect each node and it's communication with it's neighbors.  Let's make this position dependence explicit:\n",
    "\n",
    "$h^\\prime_i\\ =\\ \\mathcal{U}\\left(h_i \\ , \\ \\bigoplus\\limits_{j\\in \\mathcal{N}(i)}\\ \\mathcal{m}(h_i,\\ h_j,\\ e_{ij},\\ \\vec{x}_i,\\ \\vec{x}_j) \\right)$\n",
    "\n",
    "Where\n",
    "- $\\vec{x}_i$ is the 3D position of node $i$.\n",
    "- It's convenient to assume that $h_i$ and $e_{ij}$ do not *initially* contain position information *(but may acquire it later through interaction with neighbors)*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ee000-ca34-413f-acbe-46d83e531fa8",
   "metadata": {},
   "source": [
    "\n",
    "## Translational symmetry\n",
    "\n",
    "Suppose only the relative position of nodes matters ($\\vec{x}_i-\\vec{x}_j$).  This reduces the complexity of the model because it reduces the number of arguments we need to pass to the message function:\n",
    "\n",
    "$h^\\prime_i\\ =\\ \\mathcal{U}\\left(h_i \\ , \\ \\bigoplus\\limits_{j\\in \\mathcal{N}(i)} \\mathcal{m}(h_i,\\ h_j,\\ e_{ij},\\  \\vec{x}_i-\\vec{x}_j) \\right)$\n",
    "\n",
    "\n",
    "To simplify the notation, let's borrow the notation used in Convolutional Neural Networks:\n",
    "\n",
    "$h^\\prime_i\\ =\\ \\mathcal{U}\\left(\\sum\\limits_{j\\in \\mathcal{N}(i)} k(\\vec{x}_j-\\vec{x}_i)\\ h_j \\right)$\n",
    "\n",
    "Where\n",
    "- $k(\\vec{x}_j-\\vec{x}_i)$ is a message function which only depends on the relative physical location of nodes $i$ and $j$.  *(Also called a \"convolution kernel\".  This terminology is borrowed from CNNs, which are GNNs for graphs of regularly spaced nodes on a lattice.)*  It could return a scalar or a matrix.  *(We will consider more general message functions later.)*\n",
    "- $\\mathcal{N}(i)$ denotes the neighbors of node $i$.  *(For CNNs, $\\mathcal{N}(i)$ would represent all the nodes located a square window around node $i$ (including node $i$) as shown below.)*\n",
    "\n",
    "![CNN_neighbors_of_i](./images/CNN_neighbors_of_i.svg)\n",
    "\n",
    "NOTE: The equation above no longer describes a general graph neural network.\n",
    "But using simple notation now will help us later.\n",
    "*(More general graph neural networks are discussed at the end of this document.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901aa94d-3cb6-4eaa-8ba4-d679c629b9e9",
   "metadata": {},
   "source": [
    "## Rotational Symmetry\n",
    "\n",
    "We want to guarantee that the predictions of the GNN are consistent, even if we rotate the integrated circuit by 90 degrees. \n",
    "\n",
    "*(NOTE: Why 90-degrees?  Why not arbitrary rotations?  The laws of physics are the same at any orientation.  But it makes sense to only consider 90-degree rotations because ICs are typically manufactured with most of their wires aligned along the X or Y directions of the wafer.  Supporting arbitrary rotations would require using \"steerable\" GNNs, which are little bit more complicated to implement, and usually have more parameters.  This would add a more complexity to our GNN model, without obvious benefit.  If I'm wrong, and there is a demand for supporting arbitrary rotations, please let me know.  I can include more notes that explain how to support this.  It's not that hard.  -Andrew 2025-4-24)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0d24e-29cf-4a1e-a65b-a62b0e83dc6d",
   "metadata": {},
   "source": [
    "## Data augmentation and pooling\n",
    "\n",
    "*One simple way* to ensure rotational consistency is to use *data augmentation* and *pooling*.\n",
    "\n",
    "- *During training*, we can augment our training data set.  We can make 4 copies each graph in the training set by rotating the coordinates by 0, 90, 180, and 270 degrees, and use these rotated graphs for training.\n",
    "- At *inference time*, we *could* do the same thing: We rotate the input graph 4 times and make predictions from each rotated version of the graph.  At the end of the calculation, the conclusions from all 4 versions of the graph would be *pooled* into a single answer *(see below for how this is done)*.  This causes a 4x increase in computational burden *during inference*, but the resulting predictions are guaranteed to be consistent (\"rotationally equivariant\") regardless of the orientation of the input graph.\n",
    "\n",
    "This is the simplest way to implement equivariance.  But it is not the most powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19508074-1a22-43d4-a64d-8682cdb8ee7e",
   "metadata": {},
   "source": [
    "## Lifting\n",
    "\n",
    "So far, we have treated the rotated graphs that we get from data augmentation as independent graphs.  To improve the expressive power of the GNN model, perhaps these 4 rotated graphs could interact with each other in some way?\n",
    "\n",
    "At this point it is useful to introduce the concept of a \"lifted\" graph.\n",
    "\n",
    "![simple_graph](./images/simple_graph.svg)  *...becomes a \"lifted graph\" $\\ \\longrightarrow \\ $*  ![lifted_graph_mu_nu](./images/lifted_graph_mu_nu.svg)\n",
    "*(I will add the edges later...)*\n",
    "\n",
    "### Definition: Lifted Graphs\n",
    "A \"lifted\" graph (shown on the right) contains 4 indentical copies of the original graph.  Each node in the *lifted* graph, $\\nu$, *(denoted by dark dots in the second figure),* corresponds to a rotated version of the original graph.  To compute the attributes/embeddings of the nodes in the lifted graph (denoted $\\nu$), the coordinates ($\\vec{x}_i$) of all the nodes in the input graph, $i$, are rotated beforehand by a corresponding angle, $\\theta_\\nu$, which is one of 4 possible angles ($0, \\frac{\\pi}{2}, \\pi, \\frac{3}{2}\\pi$).\n",
    "\n",
    "Notation:\n",
    "- $\\theta_\\nu$  is the orientation corresponding to node $\\nu$ *(where $\\theta_\\nu \\in \\{0, \\frac{pi}{2}, \\pi, \\frac{3}{2}\\pi\\}$)*.\n",
    "- $\\mathsf{h}_{\\nu}$ = The attributes of node $\\nu$.  (If position dependent, then we assume these attributes are calculated assuming that the physical location of node $\\nu$ was initially rotated by $\\theta_\\nu$.)\n",
    "\n",
    "\n",
    "### Communication between nodes in lifted graphs\n",
    "\n",
    "### Simple equivariance: Independent lifted graphs\n",
    "\n",
    "During training, we *could* let data propogate through each rotated version of the original graph *independently*.  In other words, the coordinates of each input graph would initially be rotated and loaded into the corresponding nodes ($\\nu$) of the *lifted* graph.  But as data flows through the network, nodes ($\\nu, \\mu$) corresponding to different orientations of the graph ($\\theta_\\nu \\neq \\theta_\\mu$) *would not* talk to each other.  But they would all share the same message ($k()$) and update ($\\mathcal{U}()$) functions *(which are trained using all 4 graphs)*.  At inference, the predictions from each of the 4 graphs would be calculated and pooled *(see below)*.  *(This is equivalent to the \"data augmentation + pooling\" method discussed above.)*\n",
    "\n",
    "![lifted_graph_independent](./images/lifted_graph_independent.svg)\n",
    "\n",
    "### Full equivariance: Interdependent lifted graphs\n",
    "\n",
    "*More generally*, we can improve the performace of the model by allowing the nodes from *different orientations* to talk to each other during the node update process.\n",
    "\n",
    "![lifted_graph_v1](./images/lifted_graph_v1.svg)\n",
    "\n",
    "To do that, we must define a more general message passing function (convolution kernel).\n",
    "\n",
    "### Definition $\\mathsf{k}(\\vec{x},\\theta)$ \n",
    "\n",
    "$\\mathsf{k}(\\vec{x},\\theta)$ is a convolution kernel (message function) which is defined over the space of translations ($\\vec{x}$) *and* rotations ($\\theta$).\n",
    "\n",
    "However to satisfy *rotational equivariance*, this new kernel must obey *rotational* and *translational* symmetry. \n",
    "That means it must only depend on\n",
    "the difference between the node positions\n",
    "and the difference between the two angles\n",
    "$(\\theta_\\nu-\\theta_\\mu)$.  *(See below.)*\n",
    "\n",
    "\n",
    "### Update rule for node $\\nu$\n",
    "\n",
    "$\\mathsf{h}^\\prime_\\nu\\ =\\ \\mathcal{U}\\left(\\sum\\limits_{\\mu\\in\\mathcal{N}(\\nu)} \\mathsf{k}(R({\\theta_\\nu})(\\vec{x}_{j(\\mu)}-\\vec{x}_{i(\\nu)}),\\ \\theta_\\mu-\\theta_\\nu) \\ \\mathsf{h}_\\mu \\right)$\n",
    "\n",
    "Where\n",
    "- $R({\\theta_\\nu})$ is the rotation matrix corresponding to angle $\\theta_\\nu$\n",
    "- $i(\\nu)$ is the index of node $i$ from the original graph corresponding to node $\\nu$ in the \"lifted\" graph.\n",
    "- $\\mathcal{N}(\\nu)$ are the neighbors of node $\\nu$ in the *lifted graph* (shown above).  That includes *all 4* rotated versions ($\\mu$) of each of the nodes ($j$) which were neighbors of node $i(\\nu)$ in the original graph.  *(See figure below.)*\n",
    "\n",
    "***Links to videos that explain the justification for this formula in more detail are provided below.***\n",
    "*(A generalization of this formula is also provided below.)*\n",
    "\n",
    "![lifted_graph_v2.svg](./images/lifted_graph_v2.svg)\n",
    "\n",
    "**Clarification:** The coordinates that appear in the equation above ($\\vec{x}_{i(\\nu)}$) represent the *unrotated* coordinates of node $i(\\nu)$ from the original graph.  In this equation, we rotated the coordinates $(\\vec{x}_{j(\\mu)}-\\vec{x}_{i(\\nu)})$ by the angle $\\theta_\\nu$ because this is the update rule for node $\\nu$.  \n",
    "\n",
    "\n",
    "### Model complexity\n",
    "Model complexity is 4x as large the same as the original, non-equivariant GNN because the new message function $\\mathsf{k}(\\vec{x},\\theta)$ depends explicitly on $\\theta$, which can have 4 values.\n",
    "\n",
    "In spite of this, the method has been reported to perform better than a traditional GNN model with the same computational complexity *(ie. with the same number of parameters)*, trained using data-augmentation.\n",
    "\n",
    "\n",
    "### Pooling over orientations\n",
    "What can we do with the embeddings from a *lifted* graph?  It depends...\n",
    "\n",
    "**Scalar quantities:** *If the final features we want to learn from the graph ($y$) are scalars (eg. power, heat, delay, impedance),* then we can run inference on the lifted graph and compute $y$ 4 times, using the 4 different orientations of the graph.  Then we can average the resulting $y$ values together.\n",
    "\n",
    "$y^{(ave)}\\ =\\frac{1}{4}\\sum\\limits_{\\theta\\in\\{0, \\frac{pi}{2}, \\pi, \\frac{3}{2}\\pi\\}} \\ y^\\theta$\n",
    "  \n",
    "Where $y^{\\theta}$ denotes the value of $y$ computed only using nodes $\\nu$ from the rotated version of the graph (rotated by $\\theta$).  Hopefully this sentence is clear.  *(If not, see more details below.)*\n",
    "\n",
    "**Vector quantities** (eg. electromagnetic field directions) can be pooled also.  But each vector ($\\vec{y}^{\\theta}$) must be rotated back to the original orientation (using $R(-\\theta)$) before being added together.  *(Otherwise these vectors will cancel each other out when they are averaged.)*\n",
    "\n",
    "$\\vec{y}^{(ave)}\\ =\\frac{1}{4}\\sum\\limits_{\\theta\\in\\{0, \\frac{pi}{2}, \\pi, \\frac{3}{2}\\pi\\}} R(-\\theta)\\ \\vec{y}^\\theta$\n",
    "\n",
    "-------\n",
    "*DETAILS:* *Here is the formal definition of $y^{\\theta}$:*\n",
    "\n",
    "$y^{\\theta}\\ = \\ y(\\{\\mathsf{h}^{(f)}_{\\nu(i,\\theta)} | i\\in G\\})$\n",
    "\n",
    "*Where:*\n",
    "- $\\nu(i,\\theta)$ *is the index of node $\\nu$ (from the \"lifted\" graph) corresponding to node $i$ from the rotated version of the original graph (rotated by $\\theta$).*\n",
    "- $\\{\\mathsf{h}^{(f)}_{\\nu(i,\\theta)} | i\\in G\\}$ *denotes the subset of node embeddings ($\\mathsf{h}^{(f)}_\\nu$) in the lifted graph corresponding to a rotation by $\\theta$.  (The $(f)$ indicates that these are the final node embeddings after the forward calculation is finished.)  The vector we want to calculate, $\\vec{y}$, could depend on one or more of these node embeddings.  But to be as general as possible, I made $\\vec{y}()$ a function of all of the node embeddings.*\n",
    "- $G$ *denotes the original (unlifted) graph.*\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e878f-f4bb-48eb-b986-a0cf69e7f98e",
   "metadata": {},
   "source": [
    "## General equivariant GNNs with $C_4$ symmetry\n",
    "\n",
    "The update function above ignores edge attributes and assumes that nodes are updated using a simple linear convolution kernel, $k()$.\n",
    "\n",
    "As promised, here's a more general version of the equivariant GNN node update function:\n",
    "\n",
    "$\\mathsf{h}^\\prime_\\nu\\ =\\ \\mathcal{U}\\left(\\mathsf{h}_\\nu\\ ,\\ \\bigoplus\\limits_{\\mu\\in\\mathcal{N}(\\nu)}  \\ \\mathcal{m}\\left(h_{\\nu(i)},\\ h_{\\mu(j)},\\ e_{i(\\nu),j(\\mu)},\\ R({\\theta_\\nu})(\\vec{x}_{j(\\mu)}-\\vec{x}_{i(\\nu)}),\\ \\theta_\\mu-\\theta_\\nu\\right) \\right)$\n",
    "\n",
    "\n",
    "*(I have assumed that the edge attributes $e_{i,j}$ are independent of position of the nodes and are rotationally invariant.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82ee15-0a67-4333-84c2-71982e1534c9",
   "metadata": {},
   "source": [
    "## *Further reading*\n",
    "\n",
    "These notes were inspired by Erik Bekker's videos on Equivariant CNNs:\n",
    "\n",
    "- lecture 1.1  https://www.youtube.com/watch?v=z2OEyUgSH2c&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=1\n",
    "- lecture 1.2  https://www.youtube.com/watch?v=F0OxOCZwm1Q&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=2\n",
    "- lecture 1.3  https://www.youtube.com/watch?v=cWG_1IzI0uI&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=3\n",
    "- lecture 1.4  https://www.youtube.com/watch?v=X3gP1voalDE&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=4\n",
    "- lecture 1.5  https://www.youtube.com/watch?v=kTvow5-eCCQ&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=5\n",
    "- lecture 1.6  https://www.youtube.com/watch?v=mntjPJYxwTI&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=6\n",
    "- lecture 1.7  https://www.youtube.com/watch?v=erlCaoj6sTg&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=7\n",
    "\n",
    "After watching those you can skip to the video on equivariant graph neural networks:\n",
    "- lecture 3.2  https://www.youtube.com/watch?v=o-KcYASwUco&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=17\n",
    "\n",
    "*(You can skip the videos on steerable neural networks (2.1-2.7) because they are only relevant for systems\n",
    "with continuous rotational symmetry.)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
