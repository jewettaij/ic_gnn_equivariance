{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2610ee-054f-4224-b9e8-1388c7c4662c",
   "metadata": {},
   "source": [
    "# Rotationally equivariant graph neural networks for integrated circuits\n",
    "\n",
    "This file contains some rough notes on how you might apply rotational equivariance to\n",
    "a graph neural network describing an integrated circuit.  Such graphs might include information about the physical locations of the wires and circuit components.  The predictions you make from a GNN should not change if you rotate coordinates by 90 degrees or 180 degrees.  But if you use an ordinary GNN, this is not gauranteed.\n",
    "\n",
    "## Why bother?\n",
    "\n",
    "Many systems are rotationally inviariant.  Exploiting this symmetry can improve model performace and robustness.  Using a neural network model which exploits this symmetry gaurantees that the model behaves the same way regardless of the orientation of the physical position of nodes in the input graph.  It also improves performance of the model.  Here's an example of the benefits of equivarience when applied to CNNs:\n",
    "\n",
    "### Example 1\n",
    "The output of a conventional CNN is not rotationally invariant\n",
    "\n",
    "https://github.com/QUVA-Lab/escnn/raw/master/visualizations/conventional_cnn.gif\n",
    "\n",
    "### Example 2\n",
    "Output of an equivariant CNN\n",
    "\n",
    "https://github.com/QUVA-Lab/escnn/raw/master/visualizations/vectorfield.gif\n",
    "\n",
    "*(Credit: Maurice Weiler, Gabriele Cesa)*\n",
    "\n",
    "Integrated circuits are symmetric with respect to 90 degree rotations.\n",
    "This document explains how to exploit that symmetry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09055d8e-cbd3-4d14-b6c3-3d591bcf0222",
   "metadata": {},
   "source": [
    "## GNN notation\n",
    "\n",
    "![simple_graph_with_edge_attributes](./images/simple_graph_with_edge_attributes.svg)\n",
    "\n",
    "General form of the update rule that most GNNs use to update their node attributes:\n",
    "\n",
    "$h^\\prime_i\\ =\\ \\gamma\\left(h_i \\ , \\ \\bigoplus\\limits_{j\\in \\mathcal{N}(i)}\\ \\phi(h_i,\\ h_j,\\ e_{ij}) \\right)$\n",
    "\n",
    "Where:\n",
    " - $h_i$  are node attributes.  (for circuit components and points of interest along the circuit)\n",
    "\n",
    " - $e_{ij}$  are edge attributes between nodes $i$ and $j$.  ($e_{ij}$ might encode a neighboring node's physical proximity and/or whether it is directly connected by a copper interconnect.)\n",
    "\n",
    " - $\\bigoplus$  is a message aggregator *(typically a sum, $\\sum_{j\\in \\mathcal{N}(i)}$)*\n",
    "\n",
    " - $\\phi()$  calculates the message from node j to node i.  *(Typically an MLP.)*\n",
    "\n",
    " - $\\gamma()$  is the node update function, combining the aggregated messages with the attributes for node $i$.  *(Typically an MLP.)*\n",
    "\n",
    "## Introducing geometry\n",
    "In general, the 3D coordinates of each node ($\\vec{x}_i, \\vec{x}_j$) could affect each node and it's communication with it's neighbors.  Let's make this dependence explicit:\n",
    "\n",
    "$h^\\prime_i\\ =\\ \\gamma\\left(h_i \\ , \\ \\bigoplus\\limits_{j\\in \\mathcal{N}(i)}\\ \\phi(h_i,\\ h_j,\\ e_{ij},\\ \\vec{x}_i,\\ \\vec{x}_j) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ee000-ca34-413f-acbe-46d83e531fa8",
   "metadata": {},
   "source": [
    "\n",
    "## Translational symmetry\n",
    "\n",
    "Suppose only the relative position of nodes matters.  This reduces the complexity of the model because it reduces the number of arguments we need to pass to the message function:\n",
    "\n",
    "$h^\\prime_i\\ =\\ \\gamma\\left(h_i \\ , \\ \\bigoplus\\limits_{j\\in \\mathcal{N}(i)} \\phi(h_i,\\ h_j,\\ e_{ij},\\  \\vec{x}_i-\\vec{x}_j) \\right)$\n",
    "\n",
    "\n",
    "To simplify the notation, let's assume that the message between nodes $i$ and $j$ only depends on their relative position.  I will borrow the notation used in Convolutional Neural Networks.\n",
    "\n",
    "$h^\\prime_i\\ =\\ \\gamma\\left(\\sum\\limits_{j\\in \\mathcal{N}(i)} k(\\vec{x}_j-\\vec{x}_i)\\ h_j \\right)$\n",
    "\n",
    "Where\n",
    "- $\\mathcal{N}(i)$ denotes all nearby pixel locations (including the original pixel i).\n",
    "- $k(\\vec{x}_j-\\vec{x}_i)$ is a function which only depends on the relative position of nodes i and j.  *(Also called a \"convolution kernel\".  This is a generalization of CNNs, which correspond to graphs with regularly spaced nodes connected to a window of nearby nodes)*\n",
    "\n",
    "![CNN_neighbors_of_i](./images/CNN_neighbors_of_i.svg)\n",
    "\n",
    "NOTE: This equation above no longer describes a general graph neural network.\n",
    "But the simplification makes it easier to impose rotational equivariance.\n",
    "*(More general graph neural networks are discussed at the end of this document.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901aa94d-3cb6-4eaa-8ba4-d679c629b9e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## Rotational Symmetry\n",
    "\n",
    "Integrated circuits differ from point clouds or molecular graphs\n",
    "because they have discrete 90-degree rotational symmetry in the XY plane.\n",
    "Equivariance GNNs allows us to exploit this rotational symmetry to improve prediction accuracy\n",
    "and gaurantee that the the model behaves consistently when the graph coordinates are rotated.\n",
    "\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "We want to gaurantee that our predictions are consistent at different orientations\n",
    "($\\{0, \\frac{\\pi}{2}, \\pi, \\frac{3}{2}\\pi\\}$).\n",
    "\n",
    "*One way* to do this is to use data augmentation.  Make 4 copies each graph in the training set, (rotating the coordinates by 0, 90, 180, and 270 degrees), and then use these graphs for training.  *(This does not gaurantee that the resulting GNN will rotationally equivariant, but it helps.)*\n",
    "\n",
    "\n",
    "### Lifting operation\n",
    "\n",
    "Data augmentation is *similar* to training with the original *(non-augmented)* data on a \"lifted\" version of the graph.  Introducing this concept of \"lifting\" now will make it easier later when we discuss equivariance.\n",
    "\n",
    "A \"lifted\" graph contains 4 indentical copies of the original graph *(rotated by different $\\{0, \\frac{\\pi}{2}, \\pi, \\frac{3}{2}\\pi\\}$, respectively)*, that share the same model parameters *(ie. the $k()$ function)*.\n",
    "\n",
    "![simple_graph](./images/simple_graph.svg)  *...becomes $\\ \\longrightarrow \\ $*  ![lifted_graph_mu_nu](./images/lifted_graph_mu_nu.svg)\n",
    "*(I will add the edges later...)*\n",
    "\n",
    "Each node in the lifted graph, $\\nu$ (dark dots in the figure) corresponds to a rotated version of the original graph.\n",
    "\n",
    "Define:\n",
    "- $\\theta_\\nu$  is the orientation corresponding to node $\\nu$ *(where $\\theta_\\nu \\in \\{0, \\frac{pi}{2}, \\pi, \\frac{3}{2}\\pi\\}$)*.\n",
    "- $\\mathsf{h}_{\\nu}$ = The attributes of node $\\nu$ if the physical location of all the nodes in the graph had been initially been rotated by $\\theta_\\nu$.\n",
    "\n",
    "During training, we let data propogate through each rotated version of the original graph *independently*.  Each input graph is rotated.  The resulting initial node embeddings are loaded into the corresponding nodes ($\\nu, \\mu$) of the *lifted* graph.  As data flows through the network, nodes corresponding to differeint orientations of the graph do not talk to each other.  But they all share the same update function, $k()$, and the parameters of that function are learned using data from all 4 graphs.\n",
    "\n",
    "![lifted_graph_independent](./images/lifted_graph_independent.svg)\n",
    "\n",
    "One way to gaurantee equivariance, we can use the *lifted graph* at inference time.  In other words, we can make predictions using all 4 versions of the rotated original input graph.  Then we can pool the information from those 4 predictions.  *(See below.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19508074-1a22-43d4-a64d-8682cdb8ee7e",
   "metadata": {},
   "source": [
    "\n",
    "## Equivariant GNNs with $C_4$ symmetry\n",
    "\n",
    "*More generally*, to improve the performace of the model further, we can allow the nodes from different orientations to talk to each other during the update process.\n",
    "\n",
    "![lifted_graph_v1](./images/lifted_graph_v1.svg)\n",
    "\n",
    "To do that, we must define a more general message passing function (convolution kernel).\n",
    "\n",
    "### Definition $\\mathsf{k}(\\vec{x},\\theta)$ \n",
    "\n",
    "$\\mathsf{k}(\\vec{x},\\theta)$ is a convolution kernel which is defined over the space of translations ($\\vec{x}$) *and* rotations ($\\theta$).\n",
    "\n",
    "However to satisfy *rotational equivariance*, this new kernel must obey *rotational* and *translational* symmetry.  That means, it must only depend on the difference between the node positions and the two angles $(\\theta_\\nu-\\theta_\\mu)$.  *(See below.)*\n",
    "\n",
    "\n",
    "### Update rule for node $\\nu$\n",
    "\n",
    "![lifted_graph_v2.svg](./images/lifted_graph_v2.svg)\n",
    "\n",
    "$\\mathsf{h}^\\prime_\\nu\\ =\\ \\gamma\\left(\\sum\\limits_{\\mu\\in\\mathcal{N}(\\nu)} \\mathsf{k}(R_{\\theta_\\nu}(\\vec{x}_{j(\\mu)}-\\vec{x}_{i(\\nu)}),\\ \\theta_\\mu-\\theta_\\nu) \\ \\mathsf{h}_\\mu \\right)$\n",
    "\n",
    "Where\n",
    "- $R_{\\theta_\\nu}$ is the rotation matrix corresponding to angle $\\theta_\\nu$\n",
    "- $i(\\nu)$ is the index of node $i$ from the original graph corresponding to node $\\nu$ in the \"lifted\" graph.\n",
    "  \n",
    "Note: We rotated the coordinates ($\\vec{x}_j(\\mu)-\\vec{x}_i(\\nu)$) by the angle ($\\theta_\\nu$) because this equation is the update rule for node $\\nu$.  \n",
    "\n",
    "Links to videos that explain the justification for this formula in more detail are provided below.\n",
    "\n",
    "### Pooling over orientations\n",
    "*If the features we want to learn from the graph are scalars (eg. power, delay, impedance),* then at the end of the computation, we can combine the embeddings at all orientations.  At the node level, it might look like this:\n",
    "\n",
    "$h_i^{(final)}\\ =\\sum\\limits_{\\theta\\in\\{0, \\frac{pi}{2}, \\pi, \\frac{3}{2}\\pi\\}} \\mathsf{h}_{\\nu(i,\\theta)}$\n",
    "\n",
    "Where:\n",
    "- $\\nu(i,\\theta)$ is the index of node $\\nu$ (from the \"lifted\" graph) corresponding to node $i$ (from the original graph) initially rotated by $\\theta$.\n",
    "\n",
    "Vector quantities (eg. fields) can be pooled also, but the contribution from each rotated graph must be rotated back before being added together.  (Otherwise they will cancel each other out.)\n",
    "\n",
    "\n",
    "### Model complexity\n",
    "Model complexity is 4x as large the same as the original, non-equivariant GNN because the new message function $\\mathsf{k}(\\vec{x},\\theta)$ depends explicitly on $\\theta$, which can have 4 values.\n",
    "\n",
    "In spite of this, the method has been reported to perform better than a regular GNN model *(with the same number of parameters)* using traditional data-augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e878f-f4bb-48eb-b986-a0cf69e7f98e",
   "metadata": {},
   "source": [
    "## General equivariant GNNs with $C_4$ symmetry\n",
    "\n",
    "The update function above ignores edge attributes and assumes that nodes are updated using a simple linear convolution kernel, $k()$.\n",
    "\n",
    "As promised, here's a more general version of the equivariant GNN node update function:\n",
    "\n",
    "$\\mathsf{h}^\\prime_\\nu\\ =\\ \\gamma\\left(\\mathsf{h}_\\nu\\ ,\\ \\bigoplus\\limits_{\\mu\\in\\mathcal{N}(\\nu)}  \\ \\phi\\left(h_{\\nu(i)},\\ h_{\\mu(j)},\\ e_{i(\\nu),j(\\mu)},\\ R_{\\theta_\\nu}(\\vec{x}_{j(\\mu)}-\\vec{x}_{i(\\nu)}),\\ \\theta_\\mu-\\theta_\\nu\\right) \\right)$\n",
    "\n",
    "\n",
    "*(I have assumed that the edge attributes $e_{i,j}$ are independent of position of the nodes and are rotationally invariant.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82ee15-0a67-4333-84c2-71982e1534c9",
   "metadata": {},
   "source": [
    "## *Further reading*\n",
    "\n",
    "These notes were inspired by Erik Bekker's videos on Equivariant CNNs:\n",
    "\n",
    "- lecture 1.1  https://www.youtube.com/watch?v=z2OEyUgSH2c&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=1\n",
    "- lecture 1.2  https://www.youtube.com/watch?v=F0OxOCZwm1Q&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=2\n",
    "- lecture 1.3  https://www.youtube.com/watch?v=cWG_1IzI0uI&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=3\n",
    "- lecture 1.4  https://www.youtube.com/watch?v=X3gP1voalDE&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=4\n",
    "- lecture 1.5  https://www.youtube.com/watch?v=kTvow5-eCCQ&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=5\n",
    "- lecture 1.6  https://www.youtube.com/watch?v=mntjPJYxwTI&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=6\n",
    "- lecture 1.7  https://www.youtube.com/watch?v=erlCaoj6sTg&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=7\n",
    "\n",
    "After watching those you can skip to the video on equivariant graph neural networks:\n",
    "- lecture 3.2  https://www.youtube.com/watch?v=o-KcYASwUco&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=17\n",
    "\n",
    "*(You can skip the videos on steerable neural networks (2.1-2.7) because they are only relevant for systems\n",
    "with continuous rotational symmetry.)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
